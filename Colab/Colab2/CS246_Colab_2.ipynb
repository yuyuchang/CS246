{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS246 - Colab 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557",
        "colab_type": "text"
      },
      "source": [
        "# CS246 - Colab 2\n",
        "## Frequent Pattern Mining in Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId",
        "colab_type": "text"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "709afee9-a231-4a23-d76f-49f20573b968"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 56kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130387 sha256=0e4fc5fcc2d3738b959890997ef0d88ebc3e51efb0bdec012e577cec1e377cd2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "openjdk-8-jdk-headless is already the newest version (8u232-b09-0ubuntu1~18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CJ71AKe91eh",
        "colab_type": "text"
      },
      "source": [
        "Now we authenticate a Google Drive client to download the file we will be processing in our Spark job.\n",
        "\n",
        "**Make sure to follow the interactive instructions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0orRvrc1-545",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id='1dhi1F78ssqR8gE6U-AgB80ZW7V_9snX4'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('products.csv')\n",
        "\n",
        "id='1KZBNEaIyMTcsRV817us6uLZgm-Mii8oU'\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('order_products__train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ",
        "colab_type": "text"
      },
      "source": [
        "If you executed the cells above, you should be able to see the dataset we will need for this Colab under the \"Files\" tab on the left panel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr-8fK-1lmY0",
        "colab_type": "text"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOwtm2l7lePt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-LpyCLzlul6",
        "colab_type": "text"
      },
      "source": [
        "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g87iz4klwYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "6d152bfc-eed1-4085-f955-a2099f5c3400"
      },
      "source": [
        "spark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6583880d3b56:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd9ee34fe80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdwc4xKPl3Jv",
        "colab_type": "text"
      },
      "source": [
        "If you are running this Colab on the Google hosted runtime, the cell below will create a *ngrok* tunnel which will allow you to still check the Spark UI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qephEB4Tl14J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "605f4d0d-b8ce-49b6-e8e3-7679c6a80895"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-21 00:18:14--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.237.203.145, 3.223.119.4, 34.202.58.243, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.237.203.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  13.9MB/s    in 0.9s    \n",
            "\n",
            "2020-01-21 00:18:16 (13.9 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://2c19c20e.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7",
        "colab_type": "text"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3",
        "colab_type": "text"
      },
      "source": [
        "If you run successfully the setup stage, you are ready to work with the **3 Million Instacart Orders** dataset. In case you want to read more about it, check the [official Instacart blog post](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2) about it, a concise [schema description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) of the dataset, and the [download page](https://www.instacart.com/datasets/grocery-shopping-2017).\n",
        "\n",
        "In this Colab, we will be working only with a small training dataset (~131K orders) to perform fast Frequent Pattern Mining with the FP-Growth algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu-e7Ph2_ruG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "products = spark.read.csv('products.csv', header=True, inferSchema=True)\n",
        "orders = spark.read.csv('order_products__train.csv', header=True, inferSchema=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhxZZRT9syUO",
        "colab_type": "code",
        "outputId": "83509f98-9f66-4b34-96ce-bc3b76022c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "products.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- aisle_id: string (nullable = true)\n",
            " |-- department_id: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VeRYRz2s1pm",
        "colab_type": "code",
        "outputId": "b5eabc3b-3d08-4666-8225-c6270cf4796e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "orders.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- add_to_cart_order: integer (nullable = true)\n",
            " |-- reordered: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5muD_Io59CG",
        "colab_type": "text"
      },
      "source": [
        "Use the Spark Dataframe API to join 'products' and 'orders', so that you will be able to see the product names in each transaction (and not only their ids).  Then, group by the orders by 'order_id' to obtain one row per basket (i.e., set of products purchased together by one customer). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRH4o4p7s7V6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "5129d9bf-0ac4-43c5-c036-78deabcb2865"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "data = products.join(orders, products.product_id == orders.product_id).groupBy('order_id').agg(collect_list(\"product_name\").alias(\"products_name\"))\n",
        "data.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|order_id|       products_name|\n",
            "+--------+--------------------+\n",
            "|    1342|[Bag of Organic B...|\n",
            "|    1591|[Cracked Wheat, O...|\n",
            "|    4519|[Beet Apple Carro...|\n",
            "|    4935|             [Vodka]|\n",
            "|    6357|[Fresh Mozzarella...|\n",
            "|   10362|[Crinkle Cut Fren...|\n",
            "|   19204|[Lemon Lime Thirs...|\n",
            "|   29601|[Toasted Coconut ...|\n",
            "|   31035|[Bag of Organic B...|\n",
            "|   40011|[Sea Salt Macadam...|\n",
            "|   46266|[Organic Uncured ...|\n",
            "|   51607|[Major Dickason's...|\n",
            "|   58797|[Bag of Organic B...|\n",
            "|   61793|[Raspberries, Nat...|\n",
            "|   67089|[Banana, Organic ...|\n",
            "|   70863|[Bathroom Tissue,...|\n",
            "|   88674|[Unsweetened Almo...|\n",
            "|   91937|[No. 485 Gin, Tra...|\n",
            "|   92317|[Banana, Indian M...|\n",
            "|   99621|[Organic Basil, F...|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfHoTLAg6qnM",
        "colab_type": "text"
      },
      "source": [
        "In this Colab we will explore [MLlib](https://spark.apache.org/mllib/), Apache Spark's scalable machine learning library. Specifically, you can use its implementation of the [FP-Growth](https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html#fp-growth) algorithm to perform efficiently Frequent Pattern Mining in Spark.\n",
        "Use the Python example in the documentation, and train a model with \n",
        "\n",
        "```minSupport=0.01``` and ```minConfidence=0.5```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boWgxXNns089",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE\n",
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "fpGrowth = FPGrowth(itemsCol=\"products_name\", minSupport=0.01, minConfidence=0.5)\n",
        "model = fpGrowth.fit(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kpTVdfD8UiO",
        "colab_type": "text"
      },
      "source": [
        "Compute how many frequent itemsets and association rules were generated by running FP-growth.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KYgQ_URunvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a94a5859-6f35-4de2-93ed-cddbbda3e733"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "freqItemsets = model.freqItemsets\n",
        "associationRules = model.associationRules\n",
        "\n",
        "print(freqItemsets.count())\n",
        "print(associationRules.count())"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "120\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT8Lwm1VAPoN",
        "colab_type": "text"
      },
      "source": [
        "Now retrain the FP-growth model changing only \n",
        "```minsupport=0.001``` \n",
        "and compute how many frequent itemsets and association rules were generated.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4LTM9beApYn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13f7ad6f-66da-4c1d-f5a4-7d963abc304e"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "fpGrowth = FPGrowth(itemsCol=\"products_name\", minSupport=0.001, minConfidence=0.5)\n",
        "model = fpGrowth.fit(data)\n",
        "\n",
        "print(model.freqItemsets.count())\n",
        "print(model.associationRules.count())"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4444\n",
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9FOt5jRNFGt",
        "colab_type": "text"
      },
      "source": [
        "To conclude, go to Gradescope and read the remaining questions. We will ask you to inspect the resulting dataframes, and report a few results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEqWxzTCNS87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e685b2bf-66b2-4971-f44a-e1839d616452"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "freqItemsets = model.freqItemsets\n",
        "freqItemsets.sort(desc(\"freq\")).take(20)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(items=['Banana'], freq=18726),\n",
              " Row(items=['Bag of Organic Bananas'], freq=15480),\n",
              " Row(items=['Organic Strawberries'], freq=10894),\n",
              " Row(items=['Organic Baby Spinach'], freq=9784),\n",
              " Row(items=['Large Lemon'], freq=8135),\n",
              " Row(items=['Organic Avocado'], freq=7409),\n",
              " Row(items=['Organic Hass Avocado'], freq=7293),\n",
              " Row(items=['Strawberries'], freq=6494),\n",
              " Row(items=['Limes'], freq=6033),\n",
              " Row(items=['Organic Raspberries'], freq=5546),\n",
              " Row(items=['Organic Blueberries'], freq=4966),\n",
              " Row(items=['Organic Whole Milk'], freq=4908),\n",
              " Row(items=['Organic Cucumber'], freq=4613),\n",
              " Row(items=['Organic Zucchini'], freq=4589),\n",
              " Row(items=['Organic Yellow Onion'], freq=4290),\n",
              " Row(items=['Organic Garlic'], freq=4158),\n",
              " Row(items=['Seedless Red Grapes'], freq=4059),\n",
              " Row(items=['Asparagus'], freq=3868),\n",
              " Row(items=['Organic Grape Tomatoes'], freq=3823),\n",
              " Row(items=['Organic Red Onion'], freq=3818)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMKKCt5XtAWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "635a3741-e53f-4bdf-d767-5ff72299fa86"
      },
      "source": [
        "model.associationRules.take(20)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(antecedent=['Organic Kiwi', 'Organic Hass Avocado'], consequent=['Bag of Organic Bananas'], confidence=0.5459770114942529, lift=4.627719489738336),\n",
              " Row(antecedent=['Organic Raspberries', 'Organic Hass Avocado', 'Organic Strawberries'], consequent=['Bag of Organic Bananas'], confidence=0.5984251968503937, lift=5.072272070642333),\n",
              " Row(antecedent=['Organic Broccoli', 'Organic Hass Avocado'], consequent=['Bag of Organic Bananas'], confidence=0.5048231511254019, lift=4.278897986822536),\n",
              " Row(antecedent=['Organic Unsweetened Almond Milk', 'Organic Hass Avocado'], consequent=['Bag of Organic Bananas'], confidence=0.5141065830721003, lift=4.357584667849303),\n",
              " Row(antecedent=['Yellow Onions', 'Strawberries'], consequent=['Banana'], confidence=0.5357142857142857, lift=3.7536332219526702),\n",
              " Row(antecedent=['Organic Cucumber', 'Organic Hass Avocado', 'Organic Strawberries'], consequent=['Bag of Organic Bananas'], confidence=0.546875, lift=4.635330870478036),\n",
              " Row(antecedent=['Organic Navel Orange', 'Organic Hass Avocado'], consequent=['Bag of Organic Bananas'], confidence=0.5283018867924528, lift=4.477904539027839),\n",
              " Row(antecedent=['Organic Raspberries', 'Organic Hass Avocado'], consequent=['Bag of Organic Bananas'], confidence=0.521099116781158, lift=4.416853618458589),\n",
              " Row(antecedent=[\"Organic D'Anjou Pears\", 'Organic Hass Avocado'], consequent=['Bag of Organic Bananas'], confidence=0.5170454545454546, lift=4.3824946411792345),\n",
              " Row(antecedent=['Organic Navel Orange', 'Organic Raspberries'], consequent=['Bag of Organic Bananas'], confidence=0.5412186379928315, lift=4.587387356098284),\n",
              " Row(antecedent=['Organic Whole String Cheese', 'Organic Hass Avocado'], consequent=['Bag of Organic Bananas'], confidence=0.5314685314685315, lift=4.504745125675359)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2WSXPflUN76-"
      },
      "source": [
        "Once you obtained the desired results, **head over to Gradescope and submit your solution for this Colab**!"
      ]
    }
  ]
}