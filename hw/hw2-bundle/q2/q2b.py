# -*- coding: utf-8 -*-
"""q2b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U02G8hlWlagrTh3ygBuusRin4iTFYPUa
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

id='1E-voIV2ctU4Brw022Na8RHVVRGOoNkO1'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('data.txt')

id='1yXNlZWMqUcAwDScBrkFChOHJwR1FZXmI'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('c1.txt')

id='1vfovle9DgaeK0LnbQTH0j7kRaJjsvLtb'
downloaded = drive.CreateFile({'id': id})
downloaded.GetContentFile('c2.txt')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import pyspark
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf

# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

def parseVector(line):
  return np.array([float(x) for x in line.split(" ")])

def closestPoint(p, centroids, method = 'd2'):
  bestIndex = -1
  distance = float("+inf")
  for i in range(len(centroids)):
    if method == 'd2':
      temp_distance = np.sum((p - centroids[i]) ** 2)
    elif method == 'd1':
      temp_distance = np.sum(np.abs(p - centroids[i]))

    if temp_distance < distance:
      distance = temp_distance
      bestIndex = i

  return bestIndex, distance

data = spark.read.text('data.txt').rdd.map(lambda x: x[0]).map(parseVector)
c1 = np.loadtxt('c1.txt', delimiter = ' ')
c2 = np.loadtxt('c2.txt', delimiter = ' ')

MAX_ITER = 20

centroids1 = c1.copy()
centroids2 = c2.copy()

error_c1_Manhattan_distance = []
error_c2_Manhattan_distance = []

for ITER in range(MAX_ITER):
  closest = data.map(lambda p: (closestPoint(p, centroids1, 'd1')[0], (p, 1, closestPoint(p, centroids1, 'd1')[1])))
  pointStats = closest.reduceByKey(lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))
  newCentroids = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()

  error = np.sum(np.array(list(x[1][2] for x in closest.collect())))
  error_c1_Manhattan_distance.append(error)

  for (i, p) in newCentroids:
    centroids1[i] = p

for ITER in range(MAX_ITER):
  closest = data.map(lambda p: (closestPoint(p, centroids2, 'd1')[0], (p, 1, closestPoint(p, centroids2, 'd1')[1])))
  pointStats = closest.reduceByKey(lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))
  newCentroids = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()

  error = np.sum(np.array(list(x[1][2] for x in closest.collect())))
  error_c2_Manhattan_distance.append(error)

  for (i, p) in newCentroids:
    centroids2[i] = p

plt.figure()
plt.plot(list(range(1, 21)), np.array(error_c1_Manhattan_distance), label = 'c1')
plt.plot(list(range(1, 21)), np.array(error_c2_Manhattan_distance), label = 'c2')
plt.xticks(list(range(1, 21, 2)))
plt.xlabel('iteration')
plt.ylabel('cost')
plt.legend(['c1', 'c2'])
plt.title('k-means using Manhattan distance')
plt.savefig('Manhattan_distance.png')
plt.show()

print("The percentage change in c1 by using Manhattan distance is :{}".format((error_c1_Manhattan_distance[0] - error_c1_Manhattan_distance[10]) / error_c1_Manhattan_distance[0]))
print("The percentage change in c2 by using Manhattan distance is :{}".format((error_c2_Manhattan_distance[0] - error_c2_Manhattan_distance[10]) / error_c2_Manhattan_distance[0]))

