# -*- coding: utf-8 -*-
"""q2b.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-i9ofgdeic7yfkAnUQXP2nP57gc5XRZb
"""

!pip install pyspark
!pip install -U -q PyDrive
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import pyspark
from pyspark.sql import *
from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf
from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry, DenseMatrix

# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

small_data = sc.textFile('graph-small.txt')
full_data = sc.textFile('graph-full.txt')

LAMBDA = 1
NU = 1

source_dest_pair = full_data.map(lambda x: (int(x.split('\t')[0]) - 1, int(x.split('\t')[1]) - 1)).distinct()
edges = source_dest_pair.map(lambda x: (x[0], x[1], 1))
edges_transpose = source_dest_pair.map(lambda x: (x[1], x[0], 1))

L = CoordinateMatrix(edges).toBlockMatrix()
L_transpose = CoordinateMatrix(edges_transpose).toBlockMatrix()

h_init = []

for i in range(1000):
  h_init.append((i, 0, 1))

h = CoordinateMatrix(sc.parallelize(h_init)).toBlockMatrix()

a = None

for i in range(40):

  a_new = L_transpose.multiply(h)
  a_new_max = np.max(np.array(a_new.toLocalMatrix().toArray()))
  a_new_max_inverse = []
  for j in range(1000):
    a_new_max_inverse.append((j, j, 1 / a_new_max))
  a_new_max_inverse = CoordinateMatrix(sc.parallelize(a_new_max_inverse)).toBlockMatrix()
  a = a_new_max_inverse.multiply(a_new)

  h_new = L.multiply(a)
  h_new_max = np.max(np.array(h_new.toLocalMatrix().toArray()))
  h_new_max_inverse = []
  for j in range(1000):
    h_new_max_inverse.append((j, j, 1 / h_new_max))
  h_new_max_inverse = CoordinateMatrix(sc.parallelize(h_new_max_inverse)).toBlockMatrix()
  h = h_new_max_inverse.multiply(h_new)

h_numpy = np.array(h.toLocalMatrix().toArray())
a_numpy = np.array(a.toLocalMatrix().toArray())
h_min_args = np.argsort(h_numpy, axis = 0)[:5]
a_min_args = np.argsort(a_numpy, axis = 0)[:5]
h_max_args = np.argsort(-h_numpy, axis = 0)[:5]
a_max_args = np.argsort(-a_numpy, axis = 0)[:5]

print("The 5 node ids with the highest hubbiness scores:")
for args in h_max_args:
  print("Node id: {}, hubbiness score: {}".format(args[0] + 1, h_numpy[args][0][0]))

print("\n\n")

print("The 5 node ids with the lowest hubbiness scores:")
for args in h_min_args:
  print("Node id: {}, hubbiness score: {}".format(args[0] + 1, h_numpy[args][0][0]))

print("\n\n")

print("The 5 node ids with the highest authority scores:")
for args in a_max_args:
  print("Node id: {}, hubbiness score: {}".format(args[0] + 1, a_numpy[args][0][0]))

print("\n\n")

print("The 5 node ids with the lowest authority scores:")
for args in a_min_args:
  print("Node id: {}, hubbiness score: {}".format(args[0] + 1, a_numpy[args][0][0]))

print("\n\n")

